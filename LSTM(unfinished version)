# -*- coding: utf-8 -*-
"""
Created on Sat May  6 20:11:49 2017

@author: Jeremy
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import getData as gd
import sklearn.preprocessing as prep

def getPara():
    paraList={'rnn_unit':10,'input_size':5,'output_size':1,'lr':0.001}
    return paraList

def getLabel(arr):
    list0=[]
    for i in range(len(arr)-1):
        list0.append(np.mean(arr[i+1:min(i+6,len(arr))]))
    list0.append(arr[-1])
    return list0

def getGoldData():
    frm=gd.getFutureData('au')
    frm=frm[['Open','High','Low','Close','Volume']]
    frm.loc[:,'Label']=getLabel(frm['Close'].values)
    return frm.round(2).values
    
def getTrainData(batchSize=40,predictTime=5,trainEnd=600):    
    frm=getGoldData()
    batchIndex=[]
    trainSet=frm[:trainEnd]
    length=len(trainSet)
    trainSetNormal=(trainSet-np.mean(trainSet,axis=0))/np.std(trainSet,axis=0)
    trainX,trainY=[],[]
    for i in range(length-predictTime):
        if i % batchSize == 0:
            batchIndex.append(i)
        X=trainSetNormal[i:i+predictTime,:5]
        Y=trainSetNormal[i:i+predictTime,5,np.newaxis]
        trainX.append(X.tolist())
        trainY.append(Y.tolist())
    batchIndex.append(length-predictTime)
    return batchIndex,trainX,trainY

def getTestData(predictTime=5,testBegin=600):
    frm=getGoldData()
    testSet=frm[testBegin:]
    mean=np.mean(testSet,axis=0)
    std=np.std(testSet,axis=0)
    testSetNormal=(testSet-mean)/std
    length=(len(testSetNormal)+predictTime-1)//predictTime
    testX,testY=[],[]
    for i in range(length-1):
        X=testSetNormal[i*predictTime:(i+1)*predictTime,:5]
        Y=testSetNormal[i*predictTime:(i+1)*predictTime,5]
        testX.append(X)
        testY.append(Y)
    testX.append(testSetNormal[(i+1)*predictTime:,:5].tolist())
    testY.append(testSetNormal[(i+1)*predictTime:,5].tolist())
    return mean,std,testX,testY

weights={'in':np.float32(np.zeros(5)),'out':np.float32(np.zeros(5))}
biases={'in':np.float32(0),'out':np.float32(0)}
    
def LSTM(X):
    paraList=getPara()
    rnn_unit=paraList['rnn_unit']
    batchSize=tf.shape(X)[0]
    predictTime=tf.shape(X)[1]
    w_in=weights['in']
    b_in=biases['in']
    inputX=tf.reshape(X,[-1,input_size])
    input_rnn=tf.matmul(inputX,w_in)+b_in
    input_rnn=tf.reshape(input_rnn,[-1,5,rnn_unit])
    cell=tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)
    init_state=cell.zero_state(batchSize,dtype=tf.float32)
    output_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=init_state, dtype=tf.float32)
    output=tf.reshape(output_rnn,[-1,rnn_unit])
    w_out=weights['out']
    b_out=biases['out']
    pred=tf.matmul(output,w_out)+b_out
    return pred,final_states

input_size=5
output_size=1

def trainLSTM(batchSize=40,predictTime=5,trainEnd=600):
    X=tf.placeholder(tf.float32, shape=[None,predictTime,input_size])
    Y=tf.placeholder(tf.float32, shape=[None,predictTime,output_size])
    batch_index,train_x,train_y=getTrainData(batchSize,predictTime,trainEnd)
    pred,_=LSTM(X)
    loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))
    train_op=tf.train.AdamOptimizer(lr).minimize(loss)
    saver=tf.train.Saver(tf.global_variables(),max_to_keep=15)
    module_file = tf.train.latest_checkpoint()
    with tf.Session() as sess:
        saver.restore(sess, module_file)
        for i in range(2000):
            for step in range(len(batch_index)-1):
                _,loss_=sess.run([train_op,loss],feed_dict={X:train_x[batch_index[step]:batch_index[step+1]],Y:train_y[batch_index[step]:batch_index[step+1]]})
            print(i,loss_)
            if i % 200==0:
                print(saver.save(sess,'stock2.model',global_step=i))
                








