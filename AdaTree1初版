# -*- coding: utf-8 -*-
"""
Created on Thu May 11 23:09:31 2017

@author: Jeremy
"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn import tree
from sklearn import metrics
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import getData as gd
import itertools as it
import random
import myFrame as mf

def getPara():
    paraList={'RSIPeriod':[5,10,15,20,60],'diffEMAPeriod':[[5,10],[5,20],[12,26],[10,60],[20,60]],
              'MFIPeriod':[5,10,14,20,60]}
    return paraList

def getLabel(arr):
    list0=[]
    for i in range(len(arr)-1):
        list0.append(np.mean(arr[i+1:min(i+6,len(arr))]))
    list0.append(arr[-1])
    return list0

def getIndicator(trainEnd=600):
    frm=gd.getFutureData('au')
    frm=frm[['Open','High','Low','Close','Volume']]
        
    paraList=getPara()   
    frm=mf.Frames(frm)
    for i in paraList['RSIPeriod']:
        frm=frm.getRSI(i,Close='RSI'+str(i))
    for j in paraList['MFIPeriod']:
        frm=frm.getMFI(j,**{'MFI'+str(j):['High','Low','Close','Volume']})
    for k in paraList['diffEMAPeriod']:
        frm=frm.getDiffEMA(k[0],k[1],Close='diffEMA'+str(k[0])+'_'+str(k[1]))
    
    list0=getLabel(frm['Close'].values)
    frm.loc[:,'Label_0']=np.sign(list0-frm['Close'])
        
    del frm['Open']
    del frm['High']
    del frm['Low']
    del frm['Close'],
    del frm['Volume']

    return frm.dropna()

def getTrainTestData(trainEnd=600,shuffle=False):
    
    frm=getIndicator(trainEnd)
    Index=np.arange(len(frm))
    
    if shuffle==True:
        random.shuffle(Index)
        
    trainData=frm.iloc[Index[:trainEnd]]
    testData=frm.iloc[Index[trainEnd:]]
    
    trainMean=np.mean(trainData,axis=0)
    trainStd=np.std(trainData,axis=0)
    trainData=(trainData-trainMean)/trainStd
    testData=(testData-trainMean)/trainStd
    
    trainData.loc[:,'Label']=np.sign(trainData['Label_0'])
    testData.loc[:,'Label']=np.sign(testData['Label_0'])
    del trainData['Label_0']
    del testData['Label_0']
    
    return trainData,testData

def calculCorr(trainData,indicatorNum=5):

    del trainData['Label']    
    corrMat=trainData.corr()
    minCorr=1
    minCorrGroup=trainData.columns[:indicatorNum]
    
    for indicatorGroup in it.combinations(trainData.columns,indicatorNum):
        groupCorr=0
        for i in range(indicatorNum):
            for j in range(i+1,indicatorNum):
                groupCorr+=corrMat[indicatorGroup[i]][indicatorGroup[j]]
        groupCorr/=indicatorNum*(indicatorNum-1)/2
        if groupCorr<minCorr:
            minCorr=groupCorr
            minCorrGroup=indicatorGroup
    return minCorrGroup,minCorr

def plotFactorCurve(shuffle=False,indicatorNum=5):
    trainData,testData=getTrainTestData(shuffle=shuffle)
    minCorrGroup,minCorr=calculCorr(trainData,indicatorNum=indicatorNum)
    figure=plt.figure()
    ax=figure.add_subplot(1,1,1)
    color=['r','y','b','c','k']
    for factorIndex in range(indicatorNum):
        ax.plot(trainData[minCorrGroup[factorIndex]],color[factorIndex],label=minCorrGroup[factorIndex])
    plt.legend(loc='best')
    return

def testNFeat(shuffle=False,indicatorNum=5,max_depth=3,n_estimators=30):
    trainData,testData=getTrainTestData(shuffle=shuffle)
    y_train=trainData.iloc[:,-1]
    y_test=testData.iloc[:,-1]
    minCorrGroup,minCorr=calculCorr(trainData,indicatorNum=indicatorNum)
    
    trees = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth,random_state=0)
    ada = AdaBoostClassifier(base_estimator=trees,n_estimators=n_estimators, learning_rate=0.1,random_state=0)
    
    featList=[]
    for feat in minCorrGroup:
        featList.append(feat)
        
    X_train=trainData[featList].values
    X_test=testData[featList].values
    
    trees = trees.fit(X_train, y_train)
    y_train_pred = trees.predict(X_train)
    y_test_pred = trees.predict(X_test)

    tree_train = metrics.accuracy_score(y_train, y_train_pred)
    tree_test = metrics.accuracy_score(y_test, y_test_pred)
#    print('Decision tree train/test accuracies %.3f/%.3f'% (tree_train, tree_test))

    ada = ada.fit(X_train, y_train)
    y_train_pred = ada.predict(X_train)
    y_test_pred = ada.predict(X_test)

    ada_train = metrics.accuracy_score(y_train, y_train_pred) 
    ada_test = metrics.accuracy_score(y_test, y_test_pred) 
#    print('AdaBoost train/test accuracies %.3f/%.3f'% (ada_train, ada_test))
    return ada_train,ada_test,tree_train,tree_test

def plotAccuracyCurve(XLabel='n_estimators'):
    ada_trainList=[]
    ada_testList=[]
    tree_trainList=[]
    tree_testList=[]
    if XLabel=='n_estimators':
        XLabelRange=np.append(np.arange(2,20,2),np.arange(20,200,5))
    if XLabel=='indicatorNum':
        XLabelRange=np.arange(2,16)
    if XLabel=='max_depth':
        XLabelRange=np.arange(1,20)
    for i in XLabelRange:
        ada_trainShufList=[]
        ada_testShufList=[]
        tree_trainShufList=[]
        tree_testShufList=[]
        for shuffleTimes in range(30):
            if XLabel=='n_estimators':
                ada_train,ada_test,tree_train,tree_test=testNFeat(shuffle=True,indicatorNum=5,max_depth=3,n_estimators=i)
            if XLabel=='indicatorNum':
                ada_train,ada_test,tree_train,tree_test=testNFeat(shuffle=True,indicatorNum=i,max_depth=3,n_estimators=30)
            if XLabel=='max_depth':
                ada_train,ada_test,tree_train,tree_test=testNFeat(shuffle=True,indicatorNum=5,max_depth=i,n_estimators=30)
            ada_trainShufList.append(ada_train)
            ada_testShufList.append(ada_test)
            tree_trainShufList.append(tree_train)
            tree_testShufList.append(tree_test)
        ada_trainList.append(np.mean(ada_trainShufList))
        ada_testList.append(np.mean(ada_testShufList))
        tree_trainList.append(np.mean(tree_trainShufList))
        tree_testList.append(np.mean(tree_testShufList))
    #import pdb;pdb.set_trace()
    figure=plt.figure()
    ax=figure.add_subplot(1,1,1)
    ax.plot(XLabelRange,ada_trainList,'r',label='ada_train')
    ax.plot(XLabelRange,ada_testList,'y',label='ada_test')
    ax.plot(XLabelRange,tree_trainList,'b',label='tree_train')
    ax.plot(XLabelRange,tree_testList,'c',label='tree_test')
    plt.legend(loc='best')
    ax.set_xlabel(XLabel,fontsize=12)
    ax.set_ylabel('Accuracy',fontsize=12)
    return pd.DataFrame({'ada_train':ada_trainList,'ada_test':ada_testList,'tree_train':tree_trainList,
                         'tree_test':tree_testList},index=XLabelRange)

def testNFeatAveOutput(indicatorNum=15,max_depth=7,n_estimators=100):
    ada_trainShufList=[]
    ada_testShufList=[]
    tree_trainShufList=[]
    tree_testShufList=[]
    for shuffleTimes in range(30):
        ada_train,ada_test,tree_train,tree_test=testNFeat(shuffle=True,indicatorNum=indicatorNum,max_depth=max_depth,n_estimators=n_estimators)
        ada_trainShufList.append(ada_train)
        ada_testShufList.append(ada_test)
        tree_trainShufList.append(tree_train)
        tree_testShufList.append(tree_test)
#   import pdb;pdb.set_trace()
    tree_trainMean=np.mean(tree_trainShufList)
    tree_testMean=np.mean(tree_testShufList)
    ada_trainMean=np.mean(ada_trainShufList)
    ada_testMean=np.mean(ada_testShufList)
    print('Decision tree train/test accuracies %.3f/%.3f'% (tree_trainMean,tree_testMean))
    print('AdaBoost train/test accuracies %.3f/%.3f'% (ada_trainMean,ada_testMean))
    figure=plt.figure()
    ax=figure.add_subplot(1,1,1)
    ax.plot(ada_trainShufList,'r',label='ada_train')
    ax.plot(ada_testShufList,'y',label='ada_test')
    ax.plot(tree_trainShufList,'b',label='tree_train')
    ax.plot(tree_testShufList,'c',label='tree_test')
    ax.set_xlabel('Shuffletimes',fontsize=12)
    ax.set_ylabel('Accuracy',fontsize=12)
    return ada_trainShufList,ada_testShufList,tree_trainShufList,tree_testShufList
    
def testTwoFeat(firstFeat=5,secondFeat=13,shuffle=False,plot=False):
    trees = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,random_state=0)
    ada = AdaBoostClassifier(base_estimator=trees,n_estimators=30, learning_rate=0.1,random_state=0)
    trainData,testData=getTrainTestData(shuffle=shuffle)    
    y_train,X_train=trainData.iloc[:,-1],trainData.iloc[:,:-1]
    y_test,X_test=testData.iloc[:,-1],testData.iloc[:,:-1]
    feat=X_train.columns
    X_train=X_train.iloc[:,[firstFeat,secondFeat]].values
    X_test=X_test.iloc[:,[firstFeat,secondFeat]].values
#    import pdb;pdb.set_trace()                                                                                             
    trees = trees.fit(X_train, y_train)
    y_train_pred = trees.predict(X_train)
    y_test_pred = trees.predict(X_test)

    tree_train = metrics.accuracy_score(y_train, y_train_pred)
    tree_test = metrics.accuracy_score(y_test, y_test_pred)
    print('Decision tree train/test accuracies %.3f/%.3f'% (tree_train, tree_test))

    ada = ada.fit(X_train, y_train)
    y_train_pred = ada.predict(X_train)
    y_test_pred = ada.predict(X_test)

    ada_train = metrics.accuracy_score(y_train, y_train_pred) 
    ada_test = metrics.accuracy_score(y_test, y_test_pred) 
    print('AdaBoost train/test accuracies %.3f/%.3f'% (ada_train, ada_test))   

    if plot==True:
        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))
        f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(8, 3))
        cmap = plt.cm.RdYlBu
        
        for idx, clf, tt in zip([0, 1],[trees, ada],['Decision Tree', 'AdaBoost']):
            clf.fit(X_train, y_train)
            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)
            axarr[idx].contourf(xx, yy, Z, cmap=cmap,alpha=0.5)            
            axarr[idx].scatter(X_test[y_test == -1, 0],X_test[y_test == -1, 1],c='red', marker='^')
            axarr[idx].scatter(X_test[y_test == 1, 0],X_test[y_test == 1, 1],c='blue', marker='*')            
            axarr[idx].set_title(tt)

        axarr[0].set_xlabel(feat[firstFeat], fontsize=12)
        axarr[0].set_ylabel(feat[secondFeat], fontsize=12)

        plt.show()
    return
